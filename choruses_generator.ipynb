{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:36.626320Z",
     "iopub.status.busy": "2020-10-14T01:46:36.625073Z",
     "iopub.status.idle": "2020-10-14T01:46:42.677013Z",
     "shell.execute_reply": "2020-10-14T01:46:42.676435Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('final_taylor_swift_lyrics.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_P=data[data.line_number==0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TERM=[]\n",
    "for i in range(len(START_P)-1):\n",
    "    LYR=np.array(data[START_P[i]:START_P[i+1]].lyric)\n",
    "    term=LYR[0]+'\\n '\n",
    "    for t in LYR[1::]:\n",
    "        term=term+t+ ' \\n '\n",
    "    TERM.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_term=TERM[0]+' \\n'\n",
    "for t in TERM:\n",
    "    new_term=new_term+t+' \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"fuke.txt\", \"a\")\n",
    "f.write(\"Now the file has more content!\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('choruses.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can I go where you go?\\n Can we always be this close? \\n Forever and ever, ah \\n Take me out, and take me home \\n You're my, my, my, my lover \\n Can I go where you go? \\n Can we always be this close? \\n Forever and ever, ah \\n Take me out, and take me home \\n You're my, my, my, my lover \\n Ooh, look what you made me do \\n Look what you made me do \\n Look what you just made me do \\n Look what you just made me— \\n Ooh, look what you made me do \\n Look what you made me do \\n Look what you just made me do \\n Look what you just made me do \\n So oh-oh, oh-oh, oh-oh, oh-oh, oh-oh \\n You need to calm down, you're being too loud \\n And I'm just like oh-oh, oh-oh, oh-oh, oh-oh, oh-oh (Oh) \\n You need to just stop, like can you just not step on my gown? \\n You need to calm down \\n In the middle of the night, in my dreams \\n You should see the things we do, baby \\n In the middle of the night, in my dreams \\n I know I'm gonna be with you, so I take my time \\n Are you ready for it? \\n So it's gonna be forever \\n Or it's gonna go down in flames \\n You can tell me when it's over, mm \\n If the high was worth the pain \\n Got a long list of ex-lovers \\n They'll tell you I'm insane \\n 'Cause you know I love the players \\n And you love the game \\n 'Cause we're young and we're reckless \\n We'll take this way too far \\n It'll leave you breathless, hmm \\n Or with a nasty scar \\n Got a long list of ex-lovers \\n They'll tell you I'm insane \\n But I've got a blank space, baby \\n And I'll write your name \\n But I knew you \\n Dancin' in your Levi's \\n Drunk under a streetlight, I \\n I knew you \\n Hand under my sweatshirt \\n Baby, kiss it better, I \\n But we were something, don't you think so? \\n Roaring twenties, tossing pennies in the pool \\n And if my wishes came true \\n It would've been you \\n In my defense, I have none \\n For never leaving well enough alone \\n But it would've been fun \\n If you would've been the one \\n (Ooh) \\n My baby's fit like a daydream \\n Walking with his head down \\n I'm the one he's walking to \\n So call it what you want, yeah \\n Call it what you want to \\n My baby's fly like a jet stream \\n High above the whole scene \\n Loves me like I'm brand new \\n So call it what you want, yeah \\n Call it what you want to \\n Is it cool that I said all that? \\n Is it chill that you're in my head? \\n 'Cause I know that it's delicate (Delicate) \\n Is it cool that I said all that? \\n Is it too soon to do this yet? \\n 'Cause I know that it's delicate \\n Isn't it? Isn't it? Isn't it? \\n Isn't it? \\n Isn't it? Isn't it? Isn't it? \\n Isn't it... delicate? \\n And it's new, the shape of your body \\n It's blue, the feeling I've got \\n And it's ooh, whoa oh \\n It's a cruel summer \\n It's cool, that's what I tell 'em \\n No rules in breakable heaven \\n But ooh, whoa oh \\n It's a cruel summer \\n With you \\n You got that James Dean daydream look in your eye \\n And I got that red lip classic thing that you like \\n And when we go crashing down, we come back every time \\n 'Cause we never go out of style, we never go out of style \\n You got that long hair, slicked back, white t-shirt \\n And I got that good girl faith and a tight little skirt \\n And when we go crashing down, we come back every time \\n 'Cause we never go out of style, we never go out of style \\n But if I just showed up at your party \\n Would you have me? Would you want me? \\n Would you tell me to go fuck myself \\n Or lead me to the garden? \\n In the garden, would you trust me \\n If I told you it was just a summer thing? \\n I'm only seventeen, I don't know anything \\n But I know I miss you \\n You're so gorgeous \\n I can't say anything to your face \\n 'Cause look at your face \\n And I'm so furious \\n At you for making me feel this way \\n But what can I say? \\n You're gorgeous \\n I’m so sick of running as fast as I can \\n Wondering if I'd get there quicker if I was a man \\n And I'm so sick of them coming at me again \\n 'Cause if I was a man, then I'd be the man \\n I'd be the man \\n I'd be the man \\n Don't blame me, love made me crazy \\n If it doesn't, you ain't doin' it right \\n Lord, save me, my drug is my baby \\n I'll be usin' for the rest of my life \\n They say I did something bad \\n Then why's it feel so good? \\n They say I did something bad \\n But why's it feel so good? \\n Most fun I ever had \\n And I'd do it over and over and over again if I could \\n It just felt so good, good \\n I didn't have it in myself to go with grace \\n And you're the hero flying around, saving face \\n And if I'm dead to you, why are you at the wake? \\n Cursing my name, wishing I stayed \\n Look at how my tears ricochet \\n Say my name and everything just stops \\n I don't want you like a best friend \\n Only bought this dress so you could take it off \\n Take it oh, ha, ha, ha-ah \\n Carve your name into my bedpost \\n ’Cause I don't want you like a best friend \\n Only bought this dress so you could take it off \\n Take it oh, ha, ha, ha-ah \\n And they said \\n There goes the last great American dynasty \\n Who knows, if she never showed up, what could've been \\n There goes the maddest woman this town has ever seen \\n She had a marvelous time ruining \""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=[]\n",
    "for l in data.lyric.tolist():\n",
    "    if l[0]=='[':\n",
    "        S.append(1)\n",
    "    else:\n",
    "        S.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>album</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyric</th>\n",
       "      <th>line_number</th>\n",
       "      <th>release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Lover</td>\n",
       "      <td>Lover</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>reputation</td>\n",
       "      <td>Look What You Made Me Do</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>14</td>\n",
       "      <td>2017-08-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>4</td>\n",
       "      <td>Lover</td>\n",
       "      <td>You Need To Calm Down</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>11</td>\n",
       "      <td>2019-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>5</td>\n",
       "      <td>reputation</td>\n",
       "      <td>...Ready for It?</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>14</td>\n",
       "      <td>2017-09-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>6</td>\n",
       "      <td>1989 (Deluxe)</td>\n",
       "      <td>Blank Space</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>18</td>\n",
       "      <td>2014-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15865</th>\n",
       "      <td>455</td>\n",
       "      <td>Live from Paris</td>\n",
       "      <td>Lover (Live from Paris)</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16092</th>\n",
       "      <td>464</td>\n",
       "      <td>Teardrops On My Guitar - EP</td>\n",
       "      <td>Teardrops On My Guitar (Acoustic)</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>16</td>\n",
       "      <td>2007-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16133</th>\n",
       "      <td>466</td>\n",
       "      <td>Live from Paris</td>\n",
       "      <td>The Archer (Live from Paris)</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16194</th>\n",
       "      <td>468</td>\n",
       "      <td>Live from Paris</td>\n",
       "      <td>You Need To Calm Down (Live from Paris)</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16248</th>\n",
       "      <td>469</td>\n",
       "      <td>Live from SoHo - EP</td>\n",
       "      <td>Picture to Burn (Live From SoHo)</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>9</td>\n",
       "      <td>2008-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                        album  \\\n",
       "5          0                        Lover   \n",
       "53         1                   reputation   \n",
       "295        4                        Lover   \n",
       "346        5                   reputation   \n",
       "417        6                1989 (Deluxe)   \n",
       "...      ...                          ...   \n",
       "15865    455              Live from Paris   \n",
       "16092    464  Teardrops On My Guitar - EP   \n",
       "16133    466              Live from Paris   \n",
       "16194    468              Live from Paris   \n",
       "16248    469          Live from SoHo - EP   \n",
       "\n",
       "                                     song_title     lyric  line_number  \\\n",
       "5                                        Lover   [Chorus]            5   \n",
       "53                    Look What You Made Me Do   [Chorus]           14   \n",
       "295                      You Need To Calm Down   [Chorus]           11   \n",
       "346                           ...Ready for It?   [Chorus]           14   \n",
       "417                                Blank Space   [Chorus]           18   \n",
       "...                                         ...       ...          ...   \n",
       "15865                  Lover (Live from Paris)   [Chorus]            5   \n",
       "16092        Teardrops On My Guitar (Acoustic)   [Chorus]           16   \n",
       "16133             The Archer (Live from Paris)   [Chorus]           10   \n",
       "16194  You Need To Calm Down (Live from Paris)   [Chorus]           11   \n",
       "16248         Picture to Burn (Live From SoHo)   [Chorus]            9   \n",
       "\n",
       "      release_date  \n",
       "5       2019-08-16  \n",
       "53      2017-08-25  \n",
       "295     2019-06-14  \n",
       "346     2017-09-03  \n",
       "417     2014-10-27  \n",
       "...            ...  \n",
       "15865   2020-05-17  \n",
       "16092   2007-02-19  \n",
       "16133   2020-05-17  \n",
       "16194   2020-05-17  \n",
       "16248   2008-01-15  \n",
       "\n",
       "[220 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.lyric=='[Chorus]'].drop_duplicates(subset='song_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            [Verse 1]\n",
       "1    We could leave the Christmas lights up 'til Ja...\n",
       "2             And this is our place, we make the rules\n",
       "3    And there's a dazzling haze, a mysterious way ...\n",
       "Name: lyric, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.lyric[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:42.950981Z",
     "iopub.status.busy": "2020-10-14T01:46:42.950117Z",
     "iopub.status.idle": "2020-10-14T01:46:42.952715Z",
     "shell.execute_reply": "2020-10-14T01:46:42.953146Z"
    },
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:42.966713Z",
     "iopub.status.busy": "2020-10-14T01:46:42.961317Z",
     "iopub.status.idle": "2020-10-14T01:46:43.112934Z",
     "shell.execute_reply": "2020-10-14T01:46:43.113337Z"
    },
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.494847Z",
     "iopub.status.busy": "2020-10-14T01:46:44.494088Z",
     "iopub.status.idle": "2020-10-14T01:46:44.509386Z",
     "shell.execute_reply": "2020-10-14T01:46:44.508797Z"
    },
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.514806Z",
     "iopub.status.busy": "2020-10-14T01:46:44.514150Z",
     "iopub.status.idle": "2020-10-14T01:46:44.527695Z",
     "shell.execute_reply": "2020-10-14T01:46:44.528111Z"
    },
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.560283Z",
     "iopub.status.busy": "2020-10-14T01:46:44.557720Z",
     "iopub.status.idle": "2020-10-14T01:46:44.593416Z",
     "shell.execute_reply": "2020-10-14T01:46:44.592859Z"
    },
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.921732Z",
     "iopub.status.busy": "2020-10-14T01:46:44.921034Z",
     "iopub.status.idle": "2020-10-14T01:46:44.932414Z",
     "shell.execute_reply": "2020-10-14T01:46:44.932835Z"
    },
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((10, 100), (10, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.937602Z",
     "iopub.status.busy": "2020-10-14T01:46:44.936832Z",
     "iopub.status.idle": "2020-10-14T01:46:44.939374Z",
     "shell.execute_reply": "2020-10-14T01:46:44.938871Z"
    },
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.944850Z",
     "iopub.status.busy": "2020-10-14T01:46:44.944014Z",
     "iopub.status.idle": "2020-10-14T01:46:44.946130Z",
     "shell.execute_reply": "2020-10-14T01:46:44.946533Z"
    },
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:44.950589Z",
     "iopub.status.busy": "2020-10-14T01:46:44.949936Z",
     "iopub.status.idle": "2020-10-14T01:46:45.161891Z",
     "shell.execute_reply": "2020-10-14T01:46:45.162336Z"
    },
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:45.168118Z",
     "iopub.status.busy": "2020-10-14T01:46:45.167395Z",
     "iopub.status.idle": "2020-10-14T01:46:47.829144Z",
     "shell.execute_reply": "2020-10-14T01:46:47.828625Z"
    },
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100, 73) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.835818Z",
     "iopub.status.busy": "2020-10-14T01:46:47.834783Z",
     "iopub.status.idle": "2020-10-14T01:46:47.838369Z",
     "shell.execute_reply": "2020-10-14T01:46:47.837843Z"
    },
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (10, None, 256)           18688     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (10, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (10, None, 73)            74825     \n",
      "=================================================================\n",
      "Total params: 4,031,817\n",
      "Trainable params: 4,031,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.842856Z",
     "iopub.status.busy": "2020-10-14T01:46:47.842025Z",
     "iopub.status.idle": "2020-10-14T01:46:47.845423Z",
     "shell.execute_reply": "2020-10-14T01:46:47.844929Z"
    },
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.850314Z",
     "iopub.status.busy": "2020-10-14T01:46:47.849486Z",
     "iopub.status.idle": "2020-10-14T01:46:47.852877Z",
     "shell.execute_reply": "2020-10-14T01:46:47.852389Z"
    },
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 66, 68, 16, 14, 32, 32, 26,  6, 72, 52, 52,  5, 25, 18,  1, 30,\n",
       "       43, 12, 31, 47, 65, 58, 40, 30, 70,  1, 15, 43, 60, 18, 65, 14, 10,\n",
       "       10, 30, 50, 28, 26,  9, 33, 23, 44, 50, 36, 52, 33, 18,  6, 35, 14,\n",
       "        8,  1,  8,  6, 35,  2, 19, 69, 49, 24,  0,  7, 57, 20, 71, 43, 42,\n",
       "       29, 14, 37,  5, 24, 43, 60, 38, 68, 61, 19, 38, 30, 55,  7, 27,  3,\n",
       "       16, 55, 22, 17, 45, 51, 42,  7, 53, 20, 29, 34, 21, 29, 18])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.865987Z",
     "iopub.status.busy": "2020-10-14T01:46:47.865306Z",
     "iopub.status.idle": "2020-10-14T01:46:47.871046Z",
     "shell.execute_reply": "2020-10-14T01:46:47.870604Z"
    },
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.883091Z",
     "iopub.status.busy": "2020-10-14T01:46:47.882451Z",
     "iopub.status.idle": "2020-10-14T01:46:47.890277Z",
     "shell.execute_reply": "2020-10-14T01:46:47.889804Z"
    },
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.895548Z",
     "iopub.status.busy": "2020-10-14T01:46:47.894695Z",
     "iopub.status.idle": "2020-10-14T01:46:47.896964Z",
     "shell.execute_reply": "2020-10-14T01:46:47.897388Z"
    },
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.901669Z",
     "iopub.status.busy": "2020-10-14T01:46:47.900908Z",
     "iopub.status.idle": "2020-10-14T01:46:47.903457Z",
     "shell.execute_reply": "2020-10-14T01:46:47.902937Z"
    },
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:46:47.907722Z",
     "iopub.status.busy": "2020-10-14T01:46:47.907092Z",
     "iopub.status.idle": "2020-10-14T01:47:44.050116Z",
     "shell.execute_reply": "2020-10-14T01:47:44.049522Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 4.2276\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 2.5632\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 69s 2s/step - loss: 2.1601\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 78s 2s/step - loss: 2.0214\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 1.9208\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 82s 2s/step - loss: 1.8159\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 74s 2s/step - loss: 1.7215\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 66s 2s/step - loss: 1.6394\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 75s 2s/step - loss: 1.5663\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 70s 2s/step - loss: 1.4699\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 1.3822\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 1.2752\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 1.2103\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 69s 2s/step - loss: 1.0885\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.9961\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.8885\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.7868\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.6650\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.5751\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.4983\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.4115\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.3543\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.3122\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.2746\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 73s 2s/step - loss: 0.2569\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.2328\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.2158\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.1991\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1885\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.1859\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 73s 2s/step - loss: 0.1746\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.1735\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.1629\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.1551\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 73s 2s/step - loss: 0.1463\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 72s 2s/step - loss: 0.1473\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 66s 2s/step - loss: 0.1406\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.1436\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1386\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.1330\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1335\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1314\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.1302\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.1295\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.1222\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.1289\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1254\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1219\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 67s 2s/step - loss: 0.1175\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.1118\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.055058Z",
     "iopub.status.busy": "2020-10-14T01:47:44.054323Z",
     "iopub.status.idle": "2020-10-14T01:47:44.059020Z",
     "shell.execute_reply": "2020-10-14T01:47:44.058352Z"
    },
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_50'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.069516Z",
     "iopub.status.busy": "2020-10-14T01:47:44.068762Z",
     "iopub.status.idle": "2020-10-14T01:47:44.282229Z",
     "shell.execute_reply": "2020-10-14T01:47:44.281650Z"
    },
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.287368Z",
     "iopub.status.busy": "2020-10-14T01:47:44.286735Z",
     "iopub.status.idle": "2020-10-14T01:47:44.289856Z",
     "shell.execute_reply": "2020-10-14T01:47:44.290263Z"
    },
    "id": "71xa6jnYVrAN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            18688     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 73)             74825     \n",
      "=================================================================\n",
      "Total params: 4,031,817\n",
      "Trainable params: 4,031,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.297059Z",
     "iopub.status.busy": "2020-10-14T01:47:44.296432Z",
     "iopub.status.idle": "2020-10-14T01:47:44.298680Z",
     "shell.execute_reply": "2020-10-14T01:47:44.298149Z"
    },
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string,t):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 300\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = t\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.302653Z",
     "iopub.status.busy": "2020-10-14T01:47:44.302037Z",
     "iopub.status.idle": "2020-10-14T01:47:48.000324Z",
     "shell.execute_reply": "2020-10-14T01:47:48.000767Z"
    },
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call me \n",
      " You’ll never leave me \n",
      " And I’m not crazy and \n",
      " This is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n",
      " That this is really happening \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Call me\",t=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-14T01:47:44.302653Z",
     "iopub.status.busy": "2020-10-14T01:47:44.302037Z",
     "iopub.status.idle": "2020-10-14T01:47:48.000324Z",
     "shell.execute_reply": "2020-10-14T01:47:48.000767Z"
    },
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So light \n",
      " Or was it just like those promises that you made? On our last night \n",
      " I said, that's when I could finally breathe \n",
      " And by morning \n",
      " Gone was any trace of your mind \n",
      " I shouldn't be asking myself, \"Why?\" \n",
      " You shouldn't be begging for forgiveness at my feet \n",
      " You should've said no, you shou\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"So\", t=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why? \n",
      " In the garden, would you trust me, me \n",
      " Red lips and rosy cheek \n",
      " Say a mind of my friends are saying \n",
      " Girl, what are you thinking? \n",
      " You're better off \n",
      " You're better off, you're be the man \n",
      " I'd be the man \n",
      " Don'beat wet you clean \n",
      " I still got you all over me \n",
      " Drop everything now \n",
      " Meet me \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Why\", t=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me do \n",
      " Look what you made me do \n",
      " Look what you just made me— \n",
      " Ooh, ooh, ooh, I love you I love you \n",
      " And I'm down to my last grean \n",
      " This is life before you know who you're gonna be \n",
      " Fifteen \n",
      " Drop everything now \n",
      " Meet me in the pouring rain \n",
      " Kiss me on the sidewalk \n",
      " Take away the pain \n",
      " 'Cause\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Let me\", t=0.7))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
